---
title: "HDT2"
author: "Elean Rivas, Javier Alvarez"
date: "2023-02-16"
output: pdf_document
---

###Universidad del Valle de Guatemala
###Mineria de datos
###Elean Rivas - 19062
###Javier Alarez - 18051

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
peliculas <- read.csv("C:\\Users\\javie\\Desktop\\U\\2023f\\mineria\\movies.csv")
```
##1. Haga el preprocesamiento del dataset, explique qué variables no aportan información a la generación de grupos y por qué. Describa con qué variables calculará los grupos. 
```{r}
variables <- c("original_title", "originalLanguage", "homePage", "video", "actorsCharacter")
DF.variable <- data.frame(variables)
print(DF.variable)

```
Estas variables son (en nuestra opinion) variables que no ayudan con la generacion de los grupos ya que tienen caracteristicas propias que no se relacionan con las demas y/o contienen informacion no usable.

##2. Analice la tendencia al agrupamiento usando el estadístico de Hopkings y la VAT (Visual Assessment of cluster Tendency). Discuta sus resultados e impresiones. 

```{r}
library(hopkins)
peliculas <- peliculas[complete.cases(peliculas),]
popularity <- peliculas[, 'popularity']
budget <- peliculas[, 'budget']
revenue <- peliculas[,'revenue']
runtime <- peliculas[,'runtime']
voteCount <- peliculas[,'voteCount']
normd <- data.frame(popularity,budget,revenue,runtime, voteCount )
clustering <- scale(normd)

hopkins(clustering)

dataDist <- dist(clustering)
```

```{r, echo=FALSE, results='hide',fig.keep='all'} 
library(ggplot2)
library(factoextra) 
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
fviz_dist(dataDist, show_labels = F)

```

Podemos observar que el valor que retorna la funcion de hopkins esta bastante alejado de 0.5, por lo podemos decir que los datos recopilados no son aleatorios. 


##3. Determine cuál es el número de grupos a formar más adecuado para los datos que está trabajando. 
  Haga una gráfica de codo y explique la razón de la elección de la cantidad de clústeres con la que trabajará. 
  
```{r}
wss = 0
for (i in 1:10)
  wss[i] <- sum(kmeans(clustering[,1:5], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Numero de Clusters",  ylab="WSS")
```

Basado en el resultado, podemos decir que el número de clusters óptimo para analizar los datos es 6.
